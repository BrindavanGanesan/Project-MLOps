name: SageMaker Retraining & ECS Deployment Pipeline

on:
  push:
    branches:
      - main
    paths:
      - "training/**"
      - "pipeline/**"
      - "config/**"
      - "data/**"
      - "dvc.yaml"
      - "config/params.yaml"
  workflow_dispatch:
    inputs:
      reason:
        description: "Reason for triggering retrain"
        required: false

permissions:
  id-token: write
  contents: read

jobs:
  retrain_and_deploy:
    name: Retrain on SageMaker + Deploy to ECS
    runs-on: ubuntu-latest

    env:
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_ROLE_TO_ASSUME: ${{ secrets.AWS_ROLE_TO_ASSUME }}
      SAGEMAKER_ROLE_ARN: ${{ secrets.SAGEMAKER_ROLE_ARN }}
      DVC_REMOTE: s3://thebrowntiger/dvcstore
      ECR_REPOSITORY: iris-api
      ECS_CLUSTER: iris-api-cluster
      ECS_SERVICE: iris-api-service

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # ðŸ” Configure AWS credentials (OIDC)
      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc[s3] boto3 sagemaker mlflow pyyaml

      - name: Verify AWS identity
        run: aws sts get-caller-identity

    # ðŸ§ª STEP 0 â€” BASIC QUALITY CHECKS
      # Why: Quickly catches broken imports/tests before wasting money on SageMaker.
      - name: Run quick sanity tests
        run: |
          echo "Running lightweight tests before training..."
          pytest -q || exit 1

      # ðŸ§  Step 1: Retrain Model
      - name: Run DVC pipeline
        run: dvc repro

      - name: Push updated DVC outputs to remote
        run: dvc push

      - name: Upload new metrics to S3 backup
        run: |
          aws s3 cp artifacts/metrics/metrics.json s3://thebrowntiger/mlops-metrics/metrics-$(date +%Y%m%d-%H%M%S).json


       # âœ… STEP 1.5 â€” METRIC GATE
      # Why: Ensures new model performs at least as well as the previous "champion"
      - name: Compare metrics with champion
        run: |
          NEW=artifacts/metrics/metrics.json
          aws s3 cp s3://thebrowntiger/champion/metrics.json champion.json || echo '{"accuracy":0}' > champion.json
          python - <<'PY'
          import json, sys
          new=json.load(open("artifacts/metrics/metrics.json"))
          old=json.load(open("champion.json"))
          thr=-0.005  # allow up to 0.5% drop
          delta=new.get("accuracy",0)-old.get("accuracy",0)
          print(f"Champion={old.get('accuracy')} New={new.get('accuracy')} Î”={delta}")
          sys.exit(0 if delta>=thr else 1)
          PY

      - name: Promote new champion (optional)
        if: ${{ success() }}
        run: aws s3 cp artifacts/metrics/metrics.json s3://thebrowntiger/champion/metrics.json


      - name: Register latest model in MLflow
        run: |
          echo "Registering model..."
          python <<'EOF'
          import mlflow, os, sys, glob

          # Find the latest local mlruns folder copied from SageMaker
          mlruns_dirs = sorted(glob.glob("artifacts/sm-output/*/extracted/mlruns"), reverse=True)
          if not mlruns_dirs:
              print("âš ï¸ No local mlruns found â€” skipping registration.")
              sys.exit(0)

          latest_mlruns = mlruns_dirs[0]
          print(f"ðŸ“‚ Using MLflow tracking dir: {latest_mlruns}")

          mlflow.set_tracking_uri(f"file:{latest_mlruns}")
          mlflow.set_registry_uri(f"file:{latest_mlruns}")
          exp_name = "thesis-iris"

          client = mlflow.tracking.MlflowClient()
          exp = client.get_experiment_by_name(exp_name)
          if not exp:
              print(f"âš ï¸ Experiment '{exp_name}' not found in {latest_mlruns}.")
              sys.exit(0)

          runs = client.search_runs(exp.experiment_id, order_by=["attributes.start_time DESC"], max_results=1)
          if not runs:
              print("âš ï¸ No runs found â€” skipping model registration.")
              sys.exit(0)

          latest_run = runs[0]
          run_id = latest_run.info.run_id
          model_uri = f"runs:/{run_id}/model"
          print(f"Registering model from run {run_id} ({model_uri}) ...")

          try:
              result = mlflow.register_model(model_uri, "sagemaker-model")
              print("âœ… Registered model:", result.name, "version:", result.version)
          except Exception as e:
              print("âš ï¸ Skipping model registration:", e)
              sys.exit(0)
          EOF


      # ðŸ³ Step 2: Build and Push Docker Image to ECR
      - name: Log in to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build Docker image
        run: |
          IMAGE_TAG=${GITHUB_SHA::7}
          docker build -t $ECR_REPOSITORY:$IMAGE_TAG .
          docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG
          docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:latest
          echo "IMAGE_URI=${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:$IMAGE_TAG" >> $GITHUB_ENV

      - name: Push Docker image to ECR
        run: |
          docker push ${{ env.IMAGE_URI }}
          docker push ${{ steps.login-ecr.outputs.registry }}/$ECR_REPOSITORY:latest

      # ðŸš€ Step 3: Force ECS Redeployment
      - name: Force ECS service to deploy new image
        run: |
          echo "Triggering ECS redeployment..."
          aws ecs update-service \
            --cluster $ECS_CLUSTER \
            --service $ECS_SERVICE \
            --force-new-deployment \
            --region $AWS_REGION

      # âœ… Optional: Deployment confirmation
      - name: Confirm ECS service update
        run: |
          aws ecs describe-services \
            --cluster $ECS_CLUSTER \
            --services $ECS_SERVICE \
            --region $AWS_REGION \
            --query "services[0].deployments"
      
      # âœ… STEP 5 â€” POST DEPLOY SMOKE TEST
      # Why: Confirms the container is reachable and serving /predict before marking the run successful
      - name: Smoke test
        run: |
          set -e
          TASK_ARN=$(aws ecs list-tasks --cluster "$ECS_CLUSTER" --service-name "$ECS_SERVICE" --query 'taskArns[0]' --output text)
          IP=$(aws ecs describe-tasks --cluster "$ECS_CLUSTER" --tasks "$TASK_ARN" \
              --query 'tasks[0].attachments[0].details[?name==`publicIPv4Address`].value' --output text)
          echo "Hitting http://$IP:8080/healthz"
          curl -fsS "http://$IP:8080/healthz"
          echo "Hitting /predict"
          curl -fsS -X POST "http://$IP:8080/predict" -H "Content-Type: application/json" \
            -d '{"data":[[5.1,3.5,1.4,0.2]]}' | jq .
